{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5faf16",
   "metadata": {},
   "source": [
    "# Import the Require Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8751efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0fd43",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3af75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('GermanCredit.csv')\n",
    "credit_data = credit_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e135fda",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97883d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d444e221",
   "metadata": {},
   "source": [
    "# Load the Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f5e073f",
   "metadata": {},
   "source": [
    "# Re-create Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1819d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Separate target for fitting if not already done\n",
    "target = 'default'\n",
    "if target in X_train_full.columns:\n",
    "    y_train = X_train_full[target]\n",
    "    X_train_full_features = X_train_full.drop(columns=[target])\n",
    "else:\n",
    "    X_train_full_features = X_train_full # Assume target is already separate\n",
    "\n",
    "# --- Train the Pipeline ---\n",
    "print(\"\\n--- Starting Pipeline Training ---\")\n",
    "training_pipeline.fit(X_train_full_features, y_train_full) # Pass target `y` if needed by fit methods\n",
    "print(\"--- Pipeline Training Complete ---\")\n",
    "\n",
    "# --- Save the Fitted Pipeline ---\n",
    "pipeline_filename = 'full_credit_pipeline.joblib'\n",
    "pipeline_path = f\"{model_folder}/{pipeline_filename}\"\n",
    "joblib.dump(training_pipeline, pipeline_path)\n",
    "print(f\"Fitted pipeline saved to {pipeline_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib # Or however you load your pipeline\n",
    "\n",
    "# --- Assume these are loaded or available ---\n",
    "# loaded_pipeline: Your full, fitted training pipeline object (including the classifier)\n",
    "# X_test: Your raw validation features DataFrame (the input BEFORE any processing)\n",
    "# y_validation_true: Your true validation labels\n",
    "\n",
    "# --- Obtain X_val_processed ---\n",
    "try:\n",
    "    # 1. Select all steps EXCEPT the last one (the classifier)\n",
    "    #    The slicing `[:-1]` creates a temporary pipeline with only the transformer steps.\n",
    "    preprocessing_pipeline = loaded_pipeline[:-1]\n",
    "\n",
    "    # 2. Apply the transform methods of all preprocessing steps to the raw validation data\n",
    "    print(\"Applying preprocessing steps to validation data...\")\n",
    "    X_val_processed = preprocessing_pipeline.transform(X_test)\n",
    "    print(\"Preprocessing complete. X_val_processed generated.\")\n",
    "\n",
    "    # 3. (Optional but Recommended) Get Feature Names after transformation\n",
    "    #    This is often needed for SHAP plots or detailed analysis.\n",
    "    #    Requires the last step of the preprocessing pipeline to support get_feature_names_out\n",
    "    #    (like ColumnTransformer or OneHotEncoder with recent sklearn).\n",
    "    feature_names_processed = None\n",
    "    try:\n",
    "        # Access the last step of the *preprocessing part* of the pipeline\n",
    "        last_preprocessor_step_name = preprocessing_pipeline.steps[-1][0] # Get name of last step\n",
    "        last_preprocessor_object = preprocessing_pipeline.named_steps[last_preprocessor_step_name]\n",
    "\n",
    "        if hasattr(last_preprocessor_object, 'get_feature_names_out'):\n",
    "            feature_names_processed = last_preprocessor_object.get_feature_names_out()\n",
    "            print(f\"Successfully retrieved {len(feature_names_processed)} feature names after processing.\")\n",
    "        else:\n",
    "             # If the last step doesn't have it, try converting the output to a DataFrame\n",
    "             # This might work if the previous steps preserved DataFrame structure\n",
    "             if isinstance(X_val_processed, pd.DataFrame):\n",
    "                 feature_names_processed = X_val_processed.columns.tolist()\n",
    "                 print(\"Retrieved feature names from DataFrame columns.\")\n",
    "             else:\n",
    "                 print(\"Warning: Could not automatically retrieve feature names after processing.\")\n",
    "                 print(\"The last preprocessing step doesn't have 'get_feature_names_out'.\")\n",
    "                 # You might need to construct them manually based on your pipeline steps.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error retrieving feature names: {e}\")\n",
    "\n",
    "\n",
    "    # 4. (Optional) Convert X_val_processed to DataFrame if it's a NumPy array\n",
    "    #    This can make indexing easier in the analysis function, but uses more memory.\n",
    "    if not isinstance(X_val_processed, pd.DataFrame) and feature_names_processed is not None:\n",
    "        X_val_processed_df = pd.DataFrame(X_val_processed,\n",
    "                                          columns=feature_names_processed,\n",
    "                                          index=X_test.index) # Preserve original index!\n",
    "        print(\"Converted processed data to DataFrame.\")\n",
    "    elif isinstance(X_val_processed, pd.DataFrame):\n",
    "        X_val_processed_df = X_val_processed # It's already a DataFrame\n",
    "        if feature_names_processed is None: # Try to get names if not already found\n",
    "            feature_names_processed = X_val_processed_df.columns.tolist()\n",
    "    else:\n",
    "        # It's likely a NumPy array and feature names couldn't be retrieved\n",
    "        X_val_processed_df = pd.DataFrame(X_val_processed, index=X_test.index) # Create DF without column names\n",
    "        print(\"Warning: Processed data is likely a NumPy array, creating DataFrame without column names.\")\n",
    "\n",
    "\n",
    "    # --- Now you can call the analysis function ---\n",
    "    print(\"\\nCalling qualitative analysis function...\")\n",
    "    perform_qualitative_model_analysis(\n",
    "        model=loaded_pipeline.named_steps['xgb_classifier'], # Get model from full pipeline\n",
    "        X_val_processed=X_val_processed_df, # Pass the processed data (ideally DataFrame)\n",
    "        y_val=y_test, # True labels for validation set\n",
    "        X_val_raw=X_test,\n",
    "        threshold=0.4, # Your chosen threshold\n",
    "        num_samples_to_show=7,\n",
    "        class_names=['No Default', 'Default'],\n",
    "        subgroup_col='job',\n",
    "        feature_names=feature_names_processed # Pass the retrieved feature names\n",
    "    )\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"\\nError: Could not slice the pipeline. Is it a fitted Scikit-learn Pipeline object?\")\n",
    "except NotFittedError:\n",
    "     print(\"\\nError: The pipeline must be fitted before calling transform.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopsudacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
